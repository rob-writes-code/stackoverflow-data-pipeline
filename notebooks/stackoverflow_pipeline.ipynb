{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack Overflow Data Pipeline\n",
    "\n",
    "In this project, I will create a data pipeline in the Cloud using Apache Airflow.\n",
    "\n",
    "The dataset I will use for this project is an archive of Stack Overflow content.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "![flow_of_data](../screenshots/pipeline_data_flow.png)\n",
    "\n",
    "This diagram illustrates the flow of data from the source database (AWS RDS - Source Database), through to the data processing step (AWS EC2 - Apache Airflow), and finally, insertion into the analytical database (AWS RDS - Analytical Database)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EC2 Setup\n",
    "\n",
    "First, I will setup an EC2 instance on AWS.\n",
    "\n",
    "EC2 is a web service which provides computing capacity in the Cloud. \n",
    "\n",
    "I will configure Airflow to run inside my EC2 instance, so that the pipeline runs in the Cloud, independent from my local machine.\n",
    "\n",
    "First, I log into AWS and create the instance on EC2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ec2_instance](../screenshots/ec2_instance.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I connect to the EC2 instance via the terminal using a secure shell (SSH) connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ec2_connect](../screenshots/ec2_ssh_connection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect, I need to use the `pem` file which was generated when the instance was created.\n",
    "\n",
    "I save this into my current directory and then run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "ssh -i \"batching-project.pem\" ec2-user@ec2-18-130-230-199.eu-west-2.compute.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ec2_terminal](../screenshots/ec2_terminal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Airflow\n",
    "\n",
    "With a connection to the EC2 instance established, I can now install Apache Airflow inside it.\n",
    "\n",
    "I first create a venv in the EC2 and then install my project dependencies inside that using `pip`.\n",
    "\n",
    "With all my dependencies installed, I can now run the Airflow server and scheduler inside the EC2, using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "airflow db init\n",
    "\n",
    "airflow scheduler\n",
    "\n",
    "airflow webserver -p 8080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use `tmux` to run multiple panels in my terminal.\n",
    "\n",
    "This way, I can easily switch between the webserver, scheduler, EC2 terminal and local terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![airflow_ec2](../screenshots/airflow_running_ec2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the webserver is running, I can visit it using the public EC2 IP address and the Airflow port:\n",
    "\n",
    "http://18.130.230.199:8080/\n",
    "\n",
    "To log in, I must first create a user in the EC2 terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "airflow users create \\\n",
    "    --username admin \\\n",
    "    --firstname Peter \\\n",
    "    --lastname Parker \\\n",
    "    --role Admin \\\n",
    "    --password example \\\n",
    "    --email spiderman@superhero.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am now logged into Airflow running on an EC2 instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![airflow](../screenshots/airflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to RDS\n",
    "\n",
    "With Airflow now set up, I can connect to the two databases held on the AWS Relational Database Service (RDS).\n",
    "\n",
    "One database contains the raw Stack Overflow data (source), while the other is the analyical database, a currently empty database to which I will load transformed data (target).\n",
    "\n",
    "To do this, I create two new connections in Airflow, using the relevant RDS credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rds_connection](../screenshots/rds_connection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAGs\n",
    "\n",
    "With everything connected up, I can now create my first DAG.\n",
    "\n",
    "The goal of which is as follows:\n",
    "\n",
    "- Your company wants to ensure that all posts loaded into the target database have a body field that is **not empty**. This is to ensure data quality and consistency in the target database. \n",
    "- Additionally, the company wants to avoid reprocessing the same data every time the ETL process runs.\n",
    "- They are interested in these fields: `id`, `title`, `body`, `owner_user_id`, and `creation_date`.\n",
    "- The DAG should run every 15 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create new files inside my EC2, I opt to connect it to VSCode using a remote SSH extension.\n",
    "\n",
    "From here, I can easily access the EC2 and create new files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vscode](../screenshots/vscode_ssh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAG Breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will break down the DAG explaining what is happening in each part. \n",
    "\n",
    "The complete file can be viewed [here](../airflow/dags/stackoverflow_dag.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modules \n",
    "\n",
    "First, I import the necessary modules:\n",
    "\n",
    "- A combination of Postgres hook and Python operator to access the databases.\n",
    "- `Xcoms` and `Variable` to store information in Airflow, so that it can be used by separate tasks.\n",
    "- `logging` to send info to the Airflow log, which is useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
    "from airflow.models import Variable\n",
    "import logging\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Function\n",
    "\n",
    "Next, I created a Python function to extract data from the database and then push it to `xcoms`.\n",
    "\n",
    "This particular dataset has 55 million rows, which is far too much to store in `xcoms`. In addition, trying to move this amount of data at once would crash the EC2 instance, due to memory exhaustion.\n",
    "\n",
    "Therefore, I will process the data in batches of 1000 rows, every 15 minutes. \n",
    "\n",
    "In this case, I use the unique `id` primary key to mark progress in the dataset. Ids are put into ascending order, and the DAG extracts the next 1000 ids based on the id of the last uploaded item. \n",
    "\n",
    "By systematically going through the data like this, I ensure that the pipeline is idempotent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(**kwargs):\n",
    "    # postgres hook to connect to the RDS database, using connection set up earlier\n",
    "    # in Airflow\n",
    "    src_db = PostgresHook(postgres_conn_id=\"stackoverflow_source_db\")\n",
    "    src_conn = src_db.get_conn()\n",
    "\n",
    "    # store the id of the last loaded item to Variable\n",
    "    # this will be used to instruct the DAG from where to extract the next\n",
    "    # 1000 rows of data, similar to a bookmark\n",
    "    last_loaded_id = Variable.get('last_loaded_id', default_var=0)\n",
    "    logging.info(f\"Extract function: last_loaded_id from Variable is {last_loaded_id}\")\n",
    "    last_loaded_id = int(last_loaded_id)\n",
    "\n",
    "    # extraction is limited to 1000 rows, data processed in batches\n",
    "    df = pd.read_sql(\n",
    "        f'''\n",
    "        SELECT id, title, body, owner_user_id, creation_date\n",
    "        FROM posts\n",
    "        WHERE body IS NOT NULL\n",
    "        AND owner_user_id IS NOT NULL\n",
    "        AND id > {last_loaded_id}\n",
    "        ORDER BY id ASC\n",
    "        LIMIT 1000;\n",
    "        ''',\n",
    "        src_conn\n",
    "    )\n",
    "\n",
    "    # data is then pushed to xcoms, ready for use in the next task\n",
    "    kwargs['ti'].xcom_push(key='dataset', value=df.to_json())\n",
    "    logging.info(f\"Extract function: pushing dataset to Xcom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Function\n",
    "\n",
    "The load function creates a `posts` table in the analytical database.\n",
    "\n",
    "It then pulls the data stored in `xcoms` and insert it into this table.\n",
    "\n",
    "The `last_loaded_id` is updated in `Variable` ready for the next DAG run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(**kwargs):\n",
    "    # hook used to connect to analytical database\n",
    "    target_db = PostgresHook(postgres_conn_id=\"analytical_db_rds\")\n",
    "\n",
    "    # posts table created in the analytical db for filtered posts\n",
    "    create_posts_table = '''\n",
    "    CREATE TABLE IF NOT EXISTS posts (\n",
    "    id INT PRIMARY KEY,\n",
    "    title VARCHAR,\n",
    "    body VARCHAR,\n",
    "    owner_user_id INT,\n",
    "    creation_date TIMESTAMP\n",
    "    );\n",
    "    '''\n",
    "\n",
    "    # SQL to insert data into the analytical db\n",
    "    # ON CONFLICT clause ensures there will be no duplicates in the dataset\n",
    "    load_post_data = '''\n",
    "    INSERT INTO posts (id, title, body, owner_user_id, creation_date)\n",
    "    VALUES (%s, %s, %s, %s, %s)\n",
    "    ON CONFLICT (id) DO NOTHING;\n",
    "    '''\n",
    "\n",
    "    # pull the data from xcoms and load into a dataframe\n",
    "    df = pd.read_json(kwargs['ti'].xcom_pull(key='dataset'))\n",
    "    logging.info(f\"Load function: pulled dataset from Xcom. DataFrame shape is {df.shape}\")\n",
    "\n",
    "    df = df[['id', 'title', 'body', 'owner_user_id', 'creation_date']]\n",
    "    # cast 'creation_date' to a datetime object ready for load\n",
    "    # as returned as BigInt datatype from xcoms\n",
    "    df['creation_date'] = pd.to_datetime(df['creation_date'], unit='ms')\n",
    "\n",
    "    # open connection with database and insert data\n",
    "    with target_db.get_conn() as conn:\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(create_posts_table)\n",
    "                for row in df.itertuples():\n",
    "                    data = row[1:]\n",
    "                    logging.info(f'Loading data: {data}')\n",
    "                    cursor.execute(load_post_data, data)\n",
    "                conn.commit()\n",
    "\n",
    "    # update last_loaded_id Variable for next DAG run\n",
    "    max_id = int(df['id'].max() or 0)\n",
    "    Variable.set('last_loaded_id', max_id)\n",
    "    logging.info(f\"Load function: last_loaded_id pushed to Variable is {max_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Screenshots of Pipeline in Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successful run of extract and load tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dag_graph](../screenshots/dag_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log from `load_task` showing data being pulled from `xcoms` and inserted into the analytical database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![airflow_log](../screenshots/airflow_log.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of the `stackoverflow_dag`. It took quite a few runs before I got it working!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dag_overview](../screenshots/dag_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analytical database shown in TablePlus shows that data is successfully being filtered and inserted, in batches of 1,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![table_plus](../screenshots/table_plus.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
