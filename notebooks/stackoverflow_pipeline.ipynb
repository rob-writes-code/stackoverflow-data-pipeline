{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack Overflow Data Pipeline\n",
    "\n",
    "In this project, I will create a data pipeline in the Cloud using Apache Airflow.\n",
    "\n",
    "The dataset I will use for this project is an archive of Stack Overflow content.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "![flow_of_data](../screenshots/pipeline_data_flow.png)\n",
    "\n",
    "This diagram illustrates the flow of data from the source database (AWS RDS - Source Database), through to the data processing step (AWS EC2 - Apache Airflow), and finally, insertion into the analytical database (AWS RDS - Analytical Database)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EC2 Setup\n",
    "\n",
    "First, I will setup an EC2 instance on AWS.\n",
    "\n",
    "EC2 is a web service which provides computing capacity in the Cloud. \n",
    "\n",
    "I will configure Airflow to run inside my EC2 instance, so that the pipeline runs in the Cloud, independent from my local machine.\n",
    "\n",
    "First, I log into AWS and create the instance on EC2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ec2_instance](../screenshots/ec2_instance.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I connect to the EC2 instance via the terminal using a secure shell (SSH) connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ec2_connect](../screenshots/ec2_ssh_connection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect, I need to use the `pem` file which was generated when the instance was created.\n",
    "\n",
    "I save this into my current directory and then run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "ssh -i \"batching-project.pem\" ec2-user@ec2-18-130-230-199.eu-west-2.compute.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ec2_terminal](../screenshots/ec2_terminal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Airflow\n",
    "\n",
    "With a connection to the EC2 instance established, I can now install Apache Airflow inside it.\n",
    "\n",
    "I first create a venv in the EC2 and then install my project dependencies inside that using `pip`.\n",
    "\n",
    "With all my dependencies installed, I can now run the Airflow server and scheduler inside the EC2, using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "airflow db init\n",
    "\n",
    "airflow scheduler\n",
    "\n",
    "airflow webserver -p 8080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use `tmux` to run multiple panels in my terminal.\n",
    "\n",
    "This way, I can easily switch between the webserver, scheduler, EC2 terminal and local terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![airflow_ec2](../screenshots/airflow_running_ec2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the webserver is running, I can visit it using the public EC2 IP address and the Airflow port:\n",
    "\n",
    "http://18.130.230.199:8080/\n",
    "\n",
    "To log in, I must first create a user in the EC2 terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "airflow users create \\\n",
    "    --username admin \\\n",
    "    --firstname Peter \\\n",
    "    --lastname Parker \\\n",
    "    --role Admin \\\n",
    "    --password example \\\n",
    "    --email spiderman@superhero.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am now logged into Airflow running on an EC2 instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![airflow](../screenshots/airflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to RDS\n",
    "\n",
    "With Airflow now set up, I can connect to the two databases held on the AWS Relational Database Service (RDS).\n",
    "\n",
    "One database contains the raw Stack Overflow data (source), while the other is the analyical database, a currently empty database to which I will load transformed data (target).\n",
    "\n",
    "To do this, I create two new connections in Airflow, using the relevant RDS credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rds_connection](../screenshots/rds_connection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAGs\n",
    "\n",
    "With everything connected up, I can now create my first DAG.\n",
    "\n",
    "The goal of which is as follows:\n",
    "\n",
    "- Your company wants to ensure that all posts loaded into the target database have a body field that is **not empty**. This is to ensure data quality and consistency in the target database. \n",
    "- Additionally, the company wants to avoid reprocessing the same data every time the ETL process runs.\n",
    "- They are interested in these fields: `id`, `title`, `body`, `owner_user_id`, and `creation_date`.\n",
    "- The DAG should run every 15 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create new files inside my EC2, I opt to connect it to VSCode using a remote SSH extension.\n",
    "\n",
    "From here, I can easily access the EC2 and create new files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vscode](../screenshots/vscode_ssh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAG Breakdown"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
